{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "## ANS 1.\n",
        "'''\n",
        "The wine quality dataset typically contains features such as:\n",
        "\n",
        "Fixed Acidity: Determines the wine's sharpness or crispness. Important for flavor balance.\n",
        "Volatile Acidity: High levels lead to an unpleasant vinegar taste, so it negatively affects quality.\n",
        "Citric Acid: Contributes freshness and enhances flavor.\n",
        "Residual Sugar: Adds sweetness; very low or very high values may affect consumer preference.\n",
        "Chlorides: Reflects salinity; higher levels usually degrade quality.\n",
        "Free Sulfur Dioxide: Prevents microbial growth and oxidation; impacts wine preservation.\n",
        "Total Sulfur Dioxide: Overly high levels can affect taste and safety.\n",
        "Density: Indicates sugar and alcohol content, influencing body and mouthfeel.\n",
        "pH: Measures acidity; affects microbial stability and flavor.\n",
        "Sulphates: Enhance antioxidants and contribute to wine stability.\n",
        "Alcohol: Positively correlated with perceived quality due to its impact on body and flavor.\n",
        "Wine Quality Score (Target): Rated by wine tasters, serving as the target variable.'''\n",
        "\n",
        "# Ans 2.\n",
        "'''\n",
        "Techniques Used:\n",
        "\n",
        "Mean/Median/Mode Imputation: Replacing missing values with the mean (numerical), median (for skewed distributions), or mode (categorical).\n",
        "\n",
        "Advantages: Simple, fast, and retains dataset size.\n",
        "Disadvantages: Can introduce bias, ignoring feature relationships.\n",
        "K-Nearest Neighbors (KNN): Predicts missing values based on nearest neighbors.\n",
        "\n",
        "Advantages: Preserves feature relationships.\n",
        "Disadvantages: Computationally expensive for large datasets.\n",
        "Multiple Imputation by Chained Equations (MICE): Uses iterative regression to predict missing values.\n",
        "\n",
        "Advantages: Handles complex relationships well.\n",
        "Disadvantages: Computationally intensive.\n",
        "Dropping Rows/Columns: Removes rows or features with missing data.\n",
        "\n",
        "Advantages: Ensures data integrity.\n",
        "Disadvantages: Risk of losing valuable information.'''\n",
        "\n",
        "\n",
        "## Ans 3.\n",
        "'''\n",
        "Socio-economic Status (SES): Parent's education and income.\n",
        "Study Habits: Regular study patterns and time management.\n",
        "School Resources: Access to quality teaching and infrastructure.\n",
        "Mental and Physical Health: Stress levels, sleep quality, and nutrition.\n",
        "Learning Environment: Peer influence and classroom engagement.\n",
        "Statistical Analysis:\n",
        "\n",
        "Use correlation analysis to identify relationships.\n",
        "Perform regression modeling to understand the impact of predictors.\n",
        "Conduct factor analysis to reduce dimensionality and group similar variables.'''\n",
        "\n",
        "# Ans 4.\n",
        "'''\n",
        "Selection:\n",
        "\n",
        "Identify important features such as parental education, study hours, and test preparation.\n",
        "Use feature importance scores from models like Random Forest.\n",
        "\n",
        "Transformation:\n",
        "\n",
        "Normalize numerical variables for algorithms sensitive to scale.\n",
        "Encode categorical variables (e.g., one-hot encoding for gender, test preparation).\n",
        "Create interaction terms, such as combining study hours and parental support.\n",
        "\n",
        "Handling Missing Data:\n",
        "Impute missing data based on trends or patterns.\n",
        "\n",
        "Outlier Detection:\n",
        "\n",
        "Identify and treat outliers using methods like the IQR rule. '''\n",
        "\n",
        "# ans 5.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import boxcox\n",
        "\n",
        "# Load the wine quality dataset\n",
        "# Assuming the dataset is a CSV file named 'winequality.csv'\n",
        "# Modify the file path as necessary\n",
        "data = pd.read_csv('winequality.csv')\n",
        "\n",
        "\"\"\"\n",
        "Q5: Perform Exploratory Data Analysis (EDA)\n",
        "- Identify the distribution of each feature\n",
        "- Highlight features exhibiting non-normality\n",
        "\"\"\"\n",
        "\n",
        "# Display basic statistics\n",
        "print(data.describe())\n",
        "\n",
        "# Plot histograms for feature distributions\n",
        "data.hist(bins=20, figsize=(12, 10), grid=False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identify non-normal features using skewness\n",
        "skewed_features = data.skew().sort_values(ascending=False)\n",
        "print(\"\\nSkewed Features:\\n\", skewed_features)\n",
        "\n",
        "# Example: Applying log transformation to reduce skewness\n",
        "for feature in skewed_features[skewed_features > 1].index:\n",
        "    if (data[feature] > 0).all():  # Log-transformable only for positive values\n",
        "        data[feature] = np.log1p(data[feature])\n",
        "\n",
        "\"\"\"\n",
        "Q6: Perform Principal Component Analysis (PCA)\n",
        "- Reduce the number of features while preserving most of the variance\n",
        "\"\"\"\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data.drop(columns=['quality']))  # Exclude target variable\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"\\nExplained Variance Ratio by Components:\\n\", explained_variance)\n",
        "\n",
        "# Plot cumulative variance explained\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(np.cumsum(explained_variance), marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Explained Variance')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Summary:\n",
        "1. Non-normal features identified and transformed.\n",
        "2. Dimensionality reduced using PCA, retaining 95% of the variance.\n",
        "\"\"\"\n"
      ]
    }
  ]
}